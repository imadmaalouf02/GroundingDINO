{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imadmlf/GroundingDINO/blob/main/DINO_APP_BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJjh3-bJ7KUo"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IrLePIUUjn1",
        "outputId": "00bb66de-c771-4af5-c5d6-6219156592ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 27 13:55:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNJEmBMX2QF1",
        "outputId": "007f9590-0565-4d9d-9793-5ddb63d22b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uFxfyrNigr0",
        "outputId": "ab4df4ec-f320-446e-b25c-ec267d850f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GroundingDINO' already exists and is not an empty directory.\n",
            "/content/GroundingDINO\n",
            "fatal: reference is not a tree: 57535c5a79791cb76e36fdb64975271354f10251\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/imadmlf/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251\n",
        "!pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhELyLVapwwJ",
        "outputId": "961673e5-fbbe-459a-ff35-af8ee1fad0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-gc2_o7it\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-gc2_o7it\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVEDC2KXPUPl",
        "outputId": "42e426db-9c05-4102-fd99-92f5f2b0c310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2tr2ETDPU0q",
        "outputId": "e7c77a31-2c17-48a6-8a0d-5a8c5236f7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtfe6lNnVE-t",
        "outputId": "e719f75f-e07f-4564-d3fb-356e07397634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supervision==0.12.0 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision==0.12.0) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from supervision==0.12.0) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from supervision==0.12.0) (4.9.0.80)\n",
            "Requirement already satisfied: pillow<9.0.0,>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from supervision==0.12.0) (8.4.0)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from supervision==0.12.0) (6.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.1->supervision==0.12.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install supervision==0.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X71EgQVVRhG",
        "outputId": "c2f0227d-ac80-4300-ebea-c439afc61cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/weights\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "!wget -q https://github.com/imadmlf/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5lZhc1GqFJR"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoHEvjZLhw07",
        "outputId": "be5f7c58-304d-4a1c-8701-e037627ef63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kxK-E8gVplf"
      },
      "outputs": [],
      "source": [
        "GROUNDING_DINO_CONFIG_PATH = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = '/content/weights/groundingdino_swint_ogc.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THuWP_Qi_1Po"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/GroundingDINO/groundingdino\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qucyk4UbArUo",
        "outputId": "34654b37-9cc5-48df-e1f6-d497fbdc207a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model_checkpoint_path\n",
        "print(GROUNDING_DINO_CHECKPOINT_PATH)\n",
        "\n",
        "# Download the checkpoint file again if necessary\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -O {GROUNDING_DINO_CHECKPOINT_PATH}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJb5eqtv5YMC",
        "outputId": "d2c2ebf6-770c-4ed9-e354-f11878eb9d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/weights/groundingdino_swint_ogc.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pNOmGML3iYj"
      },
      "outputs": [],
      "source": [
        "from groundingdino.util.inference import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD1ybvTmV3BT",
        "outputId": "55fac2c2-017f-438d-ff8e-09a6adcafc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "GD_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mnPuyZE6ZckE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psn8T_mmIjDT",
        "outputId": "17e943c8-73fb-4511-8b3a-5cdbb0acf6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation and mask\n",
        "%%writefile App.py\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "from groundingdino.util.inference import Model\n",
        "import supervision as sv\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "def app0():\n",
        "    st.title('Image Segmentation Prompt')\n",
        "    st.write(\"Welcome to the Image Segmentation and Annotation Application. This guide will help you understand how to use the different features of this application.\")\n",
        "\n",
        "    st.header(\"Navigation Sidebar\")\n",
        "    st.write(\"\"\"\n",
        "        Use the navigation sidebar on the left to switch between different functionalities:\n",
        "        - **Home**: This page, which provides an overview of the application.\n",
        "        - **Annotate Images**: Upload images and annotate them by drawing bounding boxes around specified classes.\n",
        "        - **Mask Images**: Upload images and apply masks to specified objects within the images.\n",
        "        - **Mask and Annotate Images**: Combine both annotation and masking functionalities.\n",
        "    \"\"\")\n",
        "    # Displaying an image from GitHub\n",
        "    url1 = r'https://github.com/imadmlf/GroundingDINO/blob/main/streamlit/1.png?raw=true'\n",
        "    resp = requests.get(url1, stream=True).raw\n",
        "    image_array_heart = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image_heart = cv2.imdecode(image_array_heart, cv2.IMREAD_COLOR)\n",
        "\n",
        "    st.image(cv2.cvtColor(image_heart, cv2.COLOR_BGR2RGB), caption='Navigation sidebar image')\n",
        "\n",
        "    st.header(\"How to Use the Application\")\n",
        "    st.write(\"\"\"\n",
        "        Follow these steps to use the application:\n",
        "\n",
        "        1. **Select a Functionality**:\n",
        "            - Use the sidebar to choose between annotating images, masking images, or doing both.\n",
        "\n",
        "        2. **Upload Images**:\n",
        "            - Click the \"Upload images\" button to select and upload images from your computer. You can upload multiple images at once.\n",
        "\n",
        "        3. **Enter Classes**:\n",
        "            - In the text area provided, enter the classes you want to detect, separated by commas (e.g., \"person, chair\").\n",
        "\n",
        "        4. **Process Images**:\n",
        "            - Click the \"Annotate Images\" or \"Mask Images\" button (depending on the selected functionality) to process the uploaded images.\n",
        "\n",
        "        5. **View Results**:\n",
        "            - The original and processed images (annotated or masked) will be displayed side by side for comparison.\n",
        "    \"\"\")\n",
        "\n",
        "    st.header(\"Detailed Explanation of Each Functionality\")\n",
        "\n",
        "    st.subheader(\"Annotate Images\")\n",
        "    st.write(\"\"\"\n",
        "        In this section, you can upload images and annotate them with bounding boxes around specified classes. The steps are as follows:\n",
        "        - Upload the images.\n",
        "        - Enter the classes you want to detect.\n",
        "        - Click the \"Annotate Images\" button to see the annotated images.\n",
        "    \"\"\")\n",
        "    # Displaying an image from GitHub\n",
        "    url2 = r'https://github.com/imadmlf/GroundingDINO/blob/main/streamlit/2.PNG?raw=true'\n",
        "    resp = requests.get(url2, stream=True).raw\n",
        "\n",
        "    image_array_heart = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image_heart = cv2.imdecode(image_array_heart, cv2.IMREAD_COLOR)\n",
        "\n",
        "    st.image(cv2.cvtColor(image_heart, cv2.COLOR_BGR2RGB), caption='Annotate images')\n",
        "\n",
        "    st.subheader(\"Mask Images\")\n",
        "    st.write(\"\"\"\n",
        "        In this section, you can upload images and apply masks to specified objects within the images. The steps are as follows:\n",
        "        - Upload the images.\n",
        "        - Enter the classes you want to detect.\n",
        "        - Click the \"Mask Images\" button to see the masked images.\n",
        "    \"\"\")\n",
        "    # Displaying an image from GitHub\n",
        "    url3= r'https://github.com/imadmlf/GroundingDINO/blob/main/streamlit/3.PNG?raw=true'\n",
        "    resp = requests.get(url3, stream=True).raw\n",
        "    image_array_heart = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image_heart = cv2.imdecode(image_array_heart, cv2.IMREAD_COLOR)\n",
        "\n",
        "    st.image(cv2.cvtColor(image_heart, cv2.COLOR_BGR2RGB), caption='Mask Images')\n",
        "    st.subheader(\"Mask and Annotate Images\")\n",
        "    st.write(\"\"\"\n",
        "        This section combines both annotation and masking functionalities. The steps are as follows:\n",
        "        - Upload the images.\n",
        "        - Enter the classes you want to detect.\n",
        "        - Click the \"Annotate Images\" button to see the annotated images.\n",
        "        - Click the \"Mask Images\" button to see the masked images.\n",
        "    \"\"\")\n",
        "    # Displaying an image from GitHub\n",
        "    url4 = r'https://github.com/imadmlf/GroundingDINO/blob/main/streamlit/4.PNG?raw=true'\n",
        "    resp = requests.get(url4, stream=True).raw\n",
        "    image_array_heart = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image_heart = cv2.imdecode(image_array_heart, cv2.IMREAD_COLOR)\n",
        "\n",
        "    st.image(cv2.cvtColor(image_heart, cv2.COLOR_BGR2RGB), caption='Mask and Annotate image')\n",
        "\n",
        "    st.header(\"Notes and Tips\")\n",
        "    st.write(\"\"\"\n",
        "        - Ensure the images are clear and of good quality for better detection results.\n",
        "        - The classes should be relevant to the content of the images for accurate annotations and masks.\n",
        "    \"\"\")\n",
        "\n",
        "    st.markdown(\"### Developed by SAAD & IMAD\")\n",
        "\n",
        "\n",
        "def app1():\n",
        "    st.title('Annotated:')\n",
        "    st.title('Image Segmentation Prompt ')\n",
        "    st.write(\"This application will be for image annotating functionality.\")\n",
        "\n",
        "    # Classes input\n",
        "    CLASSES = st.text_area(\"Enter the classes (separated by commas)\", \"\").split(\", \")\n",
        "\n",
        "    # Grounding DINO config paths\n",
        "    GROUNDING_DINO_CONFIG_PATH = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
        "    GROUNDING_DINO_CHECKPOINT_PATH = '/content/weights/groundingdino_swint_ogc.pth'\n",
        "\n",
        "    # File uploader for multiple images\n",
        "    uploaded_images = st.file_uploader(\"Upload images\", type=[\"jpg\", \"jpeg\", \"png\"], accept_multiple_files=True)\n",
        "\n",
        "    # Load Grounding DINO model\n",
        "    GD_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)\n",
        "\n",
        "    # List to store annotated images\n",
        "    annotated_images = []\n",
        "\n",
        "    if uploaded_images:\n",
        "        for uploaded_image in uploaded_images:\n",
        "            image = cv2.imdecode(np.frombuffer(uploaded_image.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "            st.image(uploaded_image, caption=f\"Uploaded Image: {uploaded_image.name}\")  # Ajustez la hauteur selon vos besoins\n",
        "            if CLASSES is not None:\n",
        "                detections = GD_model.predict_with_classes(\n",
        "                    image=image,\n",
        "                    classes=CLASSES,\n",
        "                    box_threshold=0.35,\n",
        "                    text_threshold=0.25\n",
        "                )\n",
        "            else:\n",
        "                detections = GD_model.predict(image=image, box_threshold=0.35, text_threshold=0.25)\n",
        "\n",
        "                        # Imprimer les détections pour vérifier ce qui est retourné\n",
        "\n",
        "            detected_classes = set(detections.class_id)\n",
        "            detected_boxes = detections.xyxy\n",
        "            class_id = detections.class_id\n",
        "\n",
        "            # Vérifier si des objets ont été détectés\n",
        "            if detected_boxes.shape[0] > 0:\n",
        "                box_annotator = sv.BoxAnnotator()\n",
        "                annotated_frame = box_annotator.annotate(\n",
        "                    scene=image.copy(),\n",
        "                    detections=detections,\n",
        "                    skip_label=False,\n",
        "                    labels=[CLASSES[i] if CLASSES else f\"Class {i}\" for i in class_id]\n",
        "                )\n",
        "                annotated_images.append((image, annotated_frame))\n",
        "            else:\n",
        "                st.write(\"Aucun objet détecté dans l'image.\")\n",
        "    if st.button('Annotate Images'):\n",
        "        if uploaded_images:\n",
        "            # Create a figure and axes\n",
        "            fig, axes = plt.subplots(len(annotated_images), 2, figsize=(10, 5 * len(annotated_images)))\n",
        "\n",
        "            # Plot each image and its annotation\n",
        "            for idx, (original_image, annotated_image) in enumerate(annotated_images):\n",
        "                if len(annotated_images) == 1:\n",
        "                    ax1, ax2 = axes[0], axes[1]\n",
        "                else:\n",
        "                    ax1, ax2 = axes[idx]\n",
        "\n",
        "                ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "                ax1.set_title('Original Image')\n",
        "                ax1.axis('off')\n",
        "\n",
        "                ax2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
        "                ax2.set_title('Annotated Image')\n",
        "                ax2.axis('off')\n",
        "                # Afficher les classes détectées\n",
        "                detected_classes_str = ', '.join([CLASSES[i] if CLASSES else f\"Class {i}\" for i in set(detections.class_id)])\n",
        "                ax2.text(0, -20, f\"Classes détectées : {detected_classes_str}\", color='white', fontsize=12, bbox=dict(facecolor='black', alpha=0.8))\n",
        "\n",
        "\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.write(\"No images have been uploaded and annotated.\")\n",
        "\n",
        "    st.text(\"Note: This is a demo app for image annotation.\")\n",
        "    st.markdown(\"# SAAD & IMAD\")\n",
        "\n",
        "def app2():\n",
        "    st.title(\"Masked\")\n",
        "    st.title(\" Image Segmentation Prompt\")\n",
        "    st.write(\"This application will be for image masking functionality.\")\n",
        "\n",
        "    # Classes input\n",
        "    CLASSES = st.text_area(\"Enter the classes (separated by commas)\", \"person, chair\").split(\", \")\n",
        "\n",
        "    # Grounding DINO config paths\n",
        "    GROUNDING_DINO_CONFIG_PATH = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
        "    GROUNDING_DINO_CHECKPOINT_PATH = '/content/weights/groundingdino_swint_ogc.pth'\n",
        "\n",
        "    # File uploader for multiple images\n",
        "    uploaded_images = st.file_uploader(\"Upload images\", type=[\"jpg\", \"jpeg\", \"png\"], accept_multiple_files=True)\n",
        "\n",
        "    # Load Grounding DINO model\n",
        "    GD_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)\n",
        "\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    MODEL_TYPE = \"vit_h\"\n",
        "    CHECKPOINT_PATH = '/content/weights/sam_vit_h_4b8939.pth'\n",
        "\n",
        "    from segment_anything import sam_model_registry, SamPredictor\n",
        "    sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "\n",
        "    mask_predictor = SamPredictor(sam)\n",
        "\n",
        "    # Lists to store annotated images\n",
        "    annotated_images = []\n",
        "    masked_images = []\n",
        "\n",
        "    if uploaded_images:\n",
        "        for uploaded_image in uploaded_images:\n",
        "            image = cv2.imdecode(np.frombuffer(uploaded_image.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "            st.image(uploaded_image, caption=f\"Uploaded Image: {uploaded_image.name}\")  # Adjust the height as needed\n",
        "\n",
        "            detections = GD_model.predict_with_classes(\n",
        "                image=image,\n",
        "                classes=CLASSES,\n",
        "                box_threshold=0.35,\n",
        "                text_threshold=0.25\n",
        "            )\n",
        "\n",
        "            detected_boxes = detections.xyxy\n",
        "            class_ids = detections.class_id\n",
        "            box_annotator = sv.BoxAnnotator()\n",
        "            mask_annotator = sv.MaskAnnotator(color=sv.Color.blue())\n",
        "\n",
        "            # Annotate and mask each detected box\n",
        "            for box, class_id in zip(detected_boxes, class_ids):\n",
        "                box = np.array(box)\n",
        "                mask_predictor.set_image(image)\n",
        "                masks, _, _ = mask_predictor.predict(\n",
        "                    point_coords=None,\n",
        "                    point_labels=None,\n",
        "                    box=box,\n",
        "                    multimask_output=False\n",
        "                )\n",
        "\n",
        "                detections = sv.Detections(\n",
        "                    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "                    mask=masks\n",
        "                )\n",
        "                detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "                annotated_image = box_annotator.annotate(\n",
        "                    scene=image.copy(),\n",
        "                    detections=detections,\n",
        "                    skip_label=False,\n",
        "                    labels=[CLASSES[class_id]]\n",
        "                )\n",
        "                masked_image = mask_annotator.annotate(scene=annotated_image.copy(), detections=detections)\n",
        "\n",
        "                # Add the annotated and masked images to the lists\n",
        "                annotated_images.append((image, annotated_image))\n",
        "                masked_images.append((image, masked_image))\n",
        "\n",
        "    if st.button('Annotate Images'):\n",
        "        if annotated_images:\n",
        "            # Create a figure and axes\n",
        "            fig, axes = plt.subplots(len(annotated_images), 2, figsize=(10, 5 * len(annotated_images)))\n",
        "\n",
        "            # Plot each image and its annotation\n",
        "            for idx, (original_image, annotated_image) in enumerate(annotated_images):\n",
        "                if len(annotated_images) == 1:\n",
        "                    ax1, ax2 = axes\n",
        "                else:\n",
        "                    ax1, ax2 = axes[idx]\n",
        "\n",
        "                ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "                ax1.set_title('Original Image')\n",
        "                ax1.axis('off')\n",
        "\n",
        "                ax2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
        "                ax2.set_title('Annotated Image')\n",
        "                ax2.axis('off')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.write(\"No images have been uploaded and annotated.\")\n",
        "\n",
        "    if st.button('Mask Images'):\n",
        "        if masked_images:\n",
        "            # Create a figure and axes\n",
        "            fig, axes = plt.subplots(len(masked_images), 2, figsize=(10, 5 * len(masked_images)))\n",
        "\n",
        "            # Plot each image and its annotation\n",
        "            for idx, (original_image, masked_image) in enumerate(masked_images):\n",
        "                if len(masked_images) == 1:\n",
        "                    ax1, ax2 = axes\n",
        "                else:\n",
        "                    ax1, ax2 = axes[idx]\n",
        "\n",
        "                ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "                ax1.set_title('Original Image')\n",
        "                ax1.axis('off')\n",
        "\n",
        "                ax2.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))\n",
        "                ax2.set_title('Masked Image')\n",
        "                ax2.axis('off')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.write(\"No images have been uploaded and masked.\")\n",
        "\n",
        "    st.text(\"Note: This is a demo app for image masking.\")\n",
        "    st.markdown(\"# SAAD & IMAD\")\n",
        "\n",
        "def app3():\n",
        "\n",
        "    st.title('# Masked and Annotated ')\n",
        "    st.title(\"Image Segmentation Prompt\")\n",
        "    st.write(\"This application will be for image masking and annotating  functionality.\")\n",
        "\n",
        "    # Classes input\n",
        "    CLASSES = st.text_area(\"Enter the classes (separated by commas)\", \"person, chair\").split(\", \")\n",
        "\n",
        "    # Grounding DINO config paths\n",
        "    GROUNDING_DINO_CONFIG_PATH = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
        "    GROUNDING_DINO_CHECKPOINT_PATH = '/content/weights/groundingdino_swint_ogc.pth'\n",
        "\n",
        "    # File uploader for multiple images\n",
        "    uploaded_images = st.file_uploader(\"Upload images\", type=[\"jpg\", \"jpeg\", \"png\"], accept_multiple_files=True)\n",
        "\n",
        "    # Load Grounding DINO model\n",
        "    GD_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)\n",
        "\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    MODEL_TYPE = \"vit_h\"\n",
        "    CHECKPOINT_PATH = '/content/weights/sam_vit_h_4b8939.pth'\n",
        "\n",
        "    sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "    mask_predictor = SamPredictor(sam)\n",
        "\n",
        "    # Lists to store annotated images\n",
        "    annotated_images = []\n",
        "    masked_images = []\n",
        "\n",
        "    if uploaded_images:\n",
        "        for uploaded_image in uploaded_images:\n",
        "            image = cv2.imdecode(np.frombuffer(uploaded_image.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "            st.image(uploaded_image, caption=f\"Uploaded Image: {uploaded_image.name}\")\n",
        "\n",
        "            detections = GD_model.predict_with_classes(\n",
        "                image=image,\n",
        "                classes=CLASSES,\n",
        "                box_threshold=0.35,\n",
        "                text_threshold=0.25\n",
        "            )\n",
        "\n",
        "            detected_boxes = detections.xyxy\n",
        "            class_ids = detections.class_id\n",
        "            box_annotator = sv.BoxAnnotator()\n",
        "            mask_annotator = sv.MaskAnnotator(color=sv.Color.blue())\n",
        "\n",
        "            for box, class_id in zip(detected_boxes, class_ids):\n",
        "                box = np.array(box)\n",
        "                mask_predictor.set_image(image)\n",
        "                masks, _, _ = mask_predictor.predict(\n",
        "                    point_coords=None,\n",
        "                    point_labels=None,\n",
        "                    box=box,\n",
        "                    multimask_output=False\n",
        "                )\n",
        "\n",
        "                detections = sv.Detections(\n",
        "                    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "                    mask=masks\n",
        "                )\n",
        "                detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "                annotated_image = box_annotator.annotate(\n",
        "                    scene=image.copy(),\n",
        "                    detections=detections,\n",
        "                    skip_label=False,\n",
        "                    labels=[CLASSES[class_id]]\n",
        "                )\n",
        "                masked_image = mask_annotator.annotate(scene=annotated_image.copy(), detections=detections)\n",
        "\n",
        "                # Add the annotated and masked images to the lists\n",
        "                annotated_images.append((image, annotated_image))\n",
        "                masked_images.append((image, masked_image))\n",
        "\n",
        "    if st.button('Annotate Images'):\n",
        "        if annotated_images:\n",
        "            # Create a figure and axes\n",
        "            fig, axes = plt.subplots(len(annotated_images), 2, figsize=(10, 5 * len(annotated_images)))\n",
        "\n",
        "            # Plot each image and its annotation\n",
        "            for idx, (original_image, annotated_image) in enumerate(annotated_images):\n",
        "                if len(annotated_images) == 1:\n",
        "                    ax1, ax2 = axes\n",
        "                else:\n",
        "                    ax1, ax2 = axes[idx]\n",
        "\n",
        "                ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "                ax1.set_title('Original Image')\n",
        "                ax1.axis('off')\n",
        "\n",
        "                ax2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
        "                ax2.set_title('Annotated Image')\n",
        "                ax2.axis('off')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.write(\"No images have been uploaded and annotated.\")\n",
        "\n",
        "    if st.button('Mask Images'):\n",
        "        if masked_images:\n",
        "            # Create a figure and axes\n",
        "            fig, axes = plt.subplots(len(masked_images), 2, figsize=(10, 5 * len(masked_images)))\n",
        "\n",
        "            # Plot each image and its annotation\n",
        "            for idx, (original_image, masked_image) in enumerate(masked_images):\n",
        "                if len(masked_images) == 1:\n",
        "                    ax1, ax2 = axes\n",
        "                else:\n",
        "                    ax1, ax2 = axes[idx]\n",
        "\n",
        "                ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "                ax1.set_title('Original Image')\n",
        "                ax1.axis('off')\n",
        "\n",
        "                ax2.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))\n",
        "                ax2.set_title('Masked Image')\n",
        "                ax2.axis('off')\n",
        "\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.write(\"No images have been uploaded and masked.\")\n",
        "\n",
        "    st.text(\"Note: This is a demo app for image annotation.\")\n",
        "    st.markdown(\"# SAAD & IMAD\")\n",
        "\n",
        "\n",
        "PAGES = {\n",
        "    \"Home\": app0,\n",
        "    \"Annotate Images\": app1,\n",
        "    \"Mask Images\": app2,\n",
        "    \"Mask and Annotate Images\": app3\n",
        "}\n",
        "\n",
        "st.sidebar.title(\"Navigation\")\n",
        "selection = st.sidebar.radio(\"Go to\", list(PAGES.keys()))\n",
        "\n",
        "page = PAGES[selection]\n",
        "page()\n"
      ],
      "metadata": {
        "id": "jw3nlVpHSUVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98744b9a-0e1b-4352-d649-4f1b904bb7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting App.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlI546iqBsKv",
        "outputId": "3c31878b-f097-430e-85e8-f2357e8dcb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.87.136.241\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n",
        "# the code of streamlit app"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# url to run the app\n",
        "![Image Description](https://github.com/imadmlf/GroundingDINO/blob/main/streamlit/Capture.PNG?raw=true)\n"
      ],
      "metadata": {
        "id": "R8Zy-FIzQhxE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjzpSXsFBsHK",
        "outputId": "270b4c2d-aaed-45ba-a6e2-7130453491b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.87.136.241:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.63s\n",
            "your url is: https://swift-states-ask.loca.lt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "final text_encoder_type: bert-base-uncased\n",
            "final text_encoder_type: bert-base-uncased\n",
            "final text_encoder_type: bert-base-uncased\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#url to run\n",
        "! streamlit run App.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yojzAxhnhNS1",
        "outputId": "097d08ec-9107-4c25-9d4f-d100356448e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# just annotation\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "from groundingdino.util.inference import Model\n",
        "import supervision as sv\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "st.title('# Image segmentation prompt (annotation & mask) App')\n",
        "\n",
        "# Classes input\n",
        "CLASSES = st.text_area(\"Enter the classes (separated by commas)\", \"person, chair\").split(\", \")\n",
        "\n",
        "# Grounding DINO config paths\n",
        "GROUNDING_DINO_CONFIG_PATH = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = '/content/weights/groundingdino_swint_ogc.pth'\n",
        "\n",
        "# File uploader for multiple images\n",
        "uploaded_images = st.file_uploader(\"Upload images\", type=[\"jpg\", \"jpeg\", \"png\"], accept_multiple_files=True)\n",
        "\n",
        "# Load Grounding DINO model\n",
        "GD_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)\n",
        "\n",
        "# List to store annotated images\n",
        "annotated_images = []\n",
        "\n",
        "if uploaded_images:\n",
        "    for uploaded_image in uploaded_images:\n",
        "        image = cv2.imdecode(np.frombuffer(uploaded_image.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "        st.image(uploaded_image, caption=f\"Uploaded Image: {uploaded_image.name}\")  # Ajustez la hauteur selon vos besoins\n",
        "\n",
        "\n",
        "        detections = GD_model.predict_with_classes(\n",
        "            image=image,\n",
        "            classes=CLASSES,\n",
        "            box_threshold=0.35,\n",
        "            text_threshold=0.25\n",
        "        )\n",
        "\n",
        "        detected_classes = set(detections.class_id)\n",
        "        detected_boxes = detections.xyxy\n",
        "        class_id = detections.class_id\n",
        "        box_annotator = sv.BoxAnnotator()\n",
        "        annotated_frame = box_annotator.annotate(\n",
        "            scene=image.copy(),\n",
        "            detections=detections,\n",
        "            skip_label=False,\n",
        "            labels=[CLASSES[i] for i in class_id]\n",
        "        )\n",
        "\n",
        "        # Add the annotated image to the list\n",
        "        annotated_images.append((image, annotated_frame))\n",
        "\n",
        "if st.button('Annotate Images'):\n",
        "    if annotated_images:\n",
        "        # Create a figure and axes\n",
        "        fig, axes = plt.subplots(len(annotated_images), 2, figsize=(10, 5 * len(annotated_images)))\n",
        "\n",
        "        # Plot each image and its annotation\n",
        "        for idx, (original_image, annotated_image) in enumerate(annotated_images):\n",
        "            if len(annotated_images) == 1:\n",
        "                ax1, ax2 = axes[0], axes[1]\n",
        "            else:\n",
        "                ax1, ax2 = axes[idx]\n",
        "\n",
        "            ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "            ax1.set_title('Original Image')\n",
        "            ax1.axis('off')\n",
        "\n",
        "            ax2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
        "            ax2.set_title('Annotated Image')\n",
        "            ax2.axis('off')\n",
        "\n",
        "        st.pyplot(fig)\n",
        "    else:\n",
        "        st.write(\"No images have been uploaded and annotated.\")\n",
        "\n",
        "st.text(\"Note: This is a demo app for image annotation.\")\n",
        "st.markdown(\"# SAAD & IMAD\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}